{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling changepoints based on existing tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import copy\n",
    "import pickle\n",
    "import pandas\n",
    "import numpy\n",
    "import numpy.random\n",
    "\n",
    "# Old-fashioned learning\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from intensix.monitor.models import Alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA = \"../data\"\n",
    "MODELS = \"../models\"\n",
    "\n",
    "STAY_TAGS = os.path.join(DATA, \"stay_tags.pkl\")\n",
    "SOMESTAYS = os.path.join(DATA, \"stays-3-14-days\")\n",
    "\n",
    "with open(STAY_TAGS, \"rb\") as f:\n",
    "    stay_tags = pickle.load(f)\n",
    "    \n",
    "with open(SOMESTAYS, \"r\") as f:\n",
    "    somestays = []\n",
    "    for line in f:\n",
    "        somestays.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for labeling a single stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_changepoints(stayid, df, changepoints):\n",
    "    \"\"\"Adds tag-based labels.\n",
    "    \"\"\"\n",
    "    changepoints = numpy.append(changepoints, numpy.zeros_like(changepoints[:, -1:]), axis=1)\n",
    "    if stayid in stay_tags:\n",
    "        tags = sorted(\n",
    "            set([(pandas.to_datetime(tag['time']) - df.index[0]).total_seconds()//60 \n",
    "                 for tag in stay_tags[stayid]\n",
    "                 if 'deterioration' in tag['concept']]))\n",
    "        ia = 0\n",
    "        it = 0\n",
    "        while True:\n",
    "            if it == len(tags):\n",
    "                break\n",
    "            if ia == len(changepoints):\n",
    "                break\n",
    "            if changepoints[ia, 0] > tags[it]:\n",
    "                it += 1\n",
    "                continue\n",
    "            changepoints[ia, -1] = 1\n",
    "            ia += 1\n",
    "    return changepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-+++++++++++++++++++-++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-++++++++++++++++++++++++++++++++++++++++++++++++++++++++-++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-++++++++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++++-++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-+-++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++-+++++++-++++++++++++++++-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++-+-+++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++++++++++++++-+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "78252 relevant changepoints out of 167298 total (0.47)\n"
     ]
    }
   ],
   "source": [
    "POS = 0\n",
    "TOT = 0\n",
    "dataset = []\n",
    "for stayid in somestays:\n",
    "    try:\n",
    "        with open(os.path.join(DATA, \"monitor-dataset-{}.pkl\".format(stayid)),\n",
    "                  \"rb\") as f:\n",
    "            df = pickle.load(f)\n",
    "        with open(os.path.join(DATA, \"monitor-dataset-{}-changepoints.npy\".format(stayid)),\n",
    "                  \"rb\") as f:\n",
    "            changepoints = numpy.load(f)\n",
    "        print(\"+\", end=\"\")\n",
    "        labeled_changepoints = label_changepoints(stayid, df, changepoints)\n",
    "        numpy.save(os.path.join(DATA, \"monitor-dataset-{}-labeled-changepoints.npy\".format(stayid)),\n",
    "                   labeled_changepoints)\n",
    "        dataset.append(labeled_changepoints)\n",
    "        POS += int(numpy.sum(labeled_changepoints[:, -1]))\n",
    "        TOT += len(labeled_changepoints)\n",
    "    except FileNotFoundError:\n",
    "        print(\"-\", end=\"\")\n",
    "print()\n",
    "print(\"{} relevant changepoints out of {} total ({:.2f})\".format(POS, TOT, POS/TOT))\n",
    "dataset = numpy.concatenate(dataset, axis=0)\n",
    "numpy.save(os.path.join(DATA, \"labeled-changepoints.npy\"), dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline tests --- scikit-learn style classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV = 5\n",
    "TF = 1 / CV\n",
    "class_weight = {0: POS,\n",
    "                1: TOT - POS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.654100846818 0.0157867953263\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight=class_weight)\n",
    "scores = cross_val_score(model, dataset[:, 2:-1], dataset[:, -1], cv=CV, scoring='f1')\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "[[ 0.37052331  0.17085838]\n",
      " [ 0.15306002  0.30555829]]\n",
      "\n",
      "TEST:\n",
      "[[ 0.31646168  0.1711211 ]\n",
      " [ 0.16808578  0.34433144]]\n"
     ]
    }
   ],
   "source": [
    "ntrain = int((1 - TF) * len(dataset))\n",
    "trainset = dataset[:ntrain]\n",
    "testset = dataset[ntrain:]\n",
    "model.fit(trainset[:, 2:-1], trainset[:, -1])\n",
    "print(\"TRAIN:\\n{}\".format(confusion_matrix(trainset[:, -1], model.predict(trainset[:, 2:-1])) /\n",
    "                          len(trainset)))\n",
    "print(\"\\nTEST:\\n{}\".format(confusion_matrix(testset[:, -1], model.predict(testset[:, 2:-1])) /\n",
    "                           len(testset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SGDClassifier(class_weight=class_weight, tol=0.001)\n",
    "scores = cross_val_score(model, dataset[:, 2:-1], dataset[:, -1], cv=CV, scoring='f1')\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntrain = int((1 - TF) * len(dataset))\n",
    "trainset = dataset[:ntrain]\n",
    "testset = dataset[ntrain:]\n",
    "model.fit(trainset[:, 2:-1], trainset[:, -1])\n",
    "print(\"TRAIN:\\n{}\".format(confusion_matrix(trainset[:, -1], model.predict(trainset[:, 2:-1])) / \n",
    "                          len(trainset)))\n",
    "print(\"\\nTEST:\\n{}\".format(confusion_matrix(testset[:, -1], model.predict(testset[:, 2:-1])) /\n",
    "                          len(testset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "model = Alert(hidden_size=128, p=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "ntrain = int((1 - TF) * len(dataset))\n",
    "trainset = torch.from_numpy(dataset[:ntrain])\n",
    "testset = torch.from_numpy(dataset[ntrain:])\n",
    "\n",
    "def truepred(model, dset):\n",
    "    \"\"\"Returns true and predicted labels.\n",
    "    \"\"\"\n",
    "    y_true = dset[:, -1].numpy()\n",
    "    y_pred = numpy.round(model(Variable(dset[:, 2:-1]))\n",
    "                         .data.numpy()[:, 0])\n",
    "    return y_true, y_pred\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "iepoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1672(34): 0.3044 3344(34): 0.3038 5016(34): 0.3038 6688(34): 0.3037 8360(34): 0.3040 \n",
      "EPOCH 34: train loss: 0.3040, test loss: 0.3145\n",
      "batch 1672(35): 0.3026 3344(35): 0.3023 5016(35): 0.3028 6688(35): 0.3034 8360(35): 0.3040 \n",
      "EPOCH 35: train loss: 0.3040, test loss: 0.3147\n",
      "batch 1672(36): 0.3032 3344(36): 0.3034 5016(36): 0.3034 6688(36): 0.3035 8360(36): 0.3037 \n",
      "EPOCH 36: train loss: 0.3037, test loss: 0.3145\n",
      "batch 1672(37): 0.3041 3344(37): 0.3043 5016(37): 0.3042 6688(37): 0.3039 8360(37): 0.3039 \n",
      "EPOCH 37: train loss: 0.3039, test loss: 0.3143\n",
      "batch 1672(38): 0.3044 3344(38): 0.3036 5016(38): 0.3041 6688(38): 0.3038 8360(38): 0.3037 \n",
      "EPOCH 38: train loss: 0.3037, test loss: 0.3145\n",
      "batch 1672(39): 0.3017 3344(39): 0.3035 5016(39): 0.3038 6688(39): 0.3035 8360(39): 0.3033 \n",
      "EPOCH 39: train loss: 0.3033, test loss: 0.3144\n",
      "batch 1672(40): 0.3034 3344(40): 0.3038 5016(40): 0.3040 6688(40): 0.3038 8360(40): 0.3037 \n",
      "EPOCH 40: train loss: 0.3036, test loss: 0.3145\n",
      "\n",
      "TRAIN:\n",
      "[[ 0.34893677  0.18067365]\n",
      " [ 0.14427143  0.32611814]]\n",
      "F1 = 0.6675\n",
      "\n",
      "TEST:\n",
      "[[ 0.33448894  0.2083682 ]\n",
      " [ 0.14578601  0.31135684]]\n",
      "F1 = 0.6375\n",
      "\n",
      "batch 1672(41): 0.3028 3344(41): 0.3039 5016(41): 0.3038 6688(41): 0.3034 8360(41): 0.3033 \n",
      "EPOCH 41: train loss: 0.3033, test loss: 0.3148\n",
      "batch 1672(42): 0.3036 3344(42): 0.3038 5016(42): 0.3036 6688(42): 0.3035 8360(42): 0.3034 \n",
      "EPOCH 42: train loss: 0.3034, test loss: 0.3149\n",
      "batch 1672(43): 0.3014 3344(43): 0.3032 5016(43): 0.3034 6688(43): 0.3034 8360(43): 0.3034 \n",
      "EPOCH 43: train loss: 0.3034, test loss: 0.3147\n",
      "batch 1672(44): 0.3037 3344(44): 0.3031 5016(44): 0.3032 6688(44): 0.3031 8360(44): 0.3033 \n",
      "EPOCH 44: train loss: 0.3033, test loss: 0.3162\n",
      "batch 1672(45): 0.3020 3344(45): 0.3028 5016(45): 0.3034 6688(45): 0.3032 8360(45): 0.3032 \n",
      "EPOCH 45: train loss: 0.3033, test loss: 0.3140\n",
      "batch 1672(46): 0.3031 3344(46): 0.3032 5016(46): 0.3030 6688(46): 0.3030 8360(46): 0.3029 \n",
      "EPOCH 46: train loss: 0.3029, test loss: 0.3147\n",
      "batch 1672(47): 0.3015 3344(47): 0.3030 5016(47): 0.3032 6688(47): 0.3029 8360(47): 0.3028 \n",
      "EPOCH 47: train loss: 0.3028, test loss: 0.3160\n",
      "batch 1672(48): 0.3014 3344(48): 0.3025 5016(48): 0.3026 6688(48): 0.3025 8360(48): 0.3028 \n",
      "EPOCH 48: train loss: 0.3028, test loss: 0.3141\n",
      "batch 1672(49): 0.3023 3344(49): 0.3026 5016(49): 0.3027 6688(49): 0.3028 8360(49): 0.3028 \n",
      "EPOCH 49: train loss: 0.3028, test loss: 0.3140\n",
      "batch 1672(50): 0.3005 3344(50): 0.3016 5016(50): 0.3020 6688(50): 0.3020 8360(50): 0.3026 \n",
      "EPOCH 50: train loss: 0.3026, test loss: 0.3139\n",
      "\n",
      "TRAIN:\n",
      "[[ 0.36663728  0.16297315]\n",
      " [ 0.15826596  0.31212361]]\n",
      "F1 = 0.6602\n",
      "\n",
      "TEST:\n",
      "[[ 0.35107591  0.19178123]\n",
      " [ 0.16126718  0.29587567]]\n",
      "F1 = 0.6263\n",
      "\n",
      "batch 1672(51): 0.3028 3344(51): 0.3025 5016(51): 0.3022 6688(51): 0.3020 8360(51): 0.3023 \n",
      "EPOCH 51: train loss: 0.3023, test loss: 0.3135\n",
      "batch 1672(52): 0.3018 3344(52): 0.3021 5016(52): 0.3025 6688(52): 0.3021 8360(52): 0.3026 \n",
      "EPOCH 52: train loss: 0.3026, test loss: 0.3152\n",
      "batch 1672(53): 0.3041 3344(53): 0.3043 5016(53): 0.3033 6688(53): 0.3032 8360(53): 0.3026 \n",
      "EPOCH 53: train loss: 0.3026, test loss: 0.3141\n",
      "batch 1672(54): 0.3023 3344(54): 0.3029 5016(54): 0.3030 6688(54): 0.3022 8360(54): 0.3025 \n",
      "EPOCH 54: train loss: 0.3025, test loss: 0.3149\n",
      "batch 1672(55): 0.3022 3344(55): 0.3023 5016(55): 0.3025 6688(55): 0.3025 8360(55): 0.3026 \n",
      "EPOCH 55: train loss: 0.3026, test loss: 0.3137\n",
      "batch 1672(56): 0.3020 3344(56): 0.3024 5016(56): 0.3024 6688(56): 0.3026 8360(56): 0.3023 \n",
      "EPOCH 56: train loss: 0.3023, test loss: 0.3137\n",
      "batch 1672(57): 0.3027 3344(57): 0.3020 5016(57): 0.3024 6688(57): 0.3022 8360(57): 0.3025 \n",
      "EPOCH 57: train loss: 0.3026, test loss: 0.3135\n",
      "batch 1672(58): 0.3022 3344(58): 0.3022 5016(58): 0.3023 6688(58): 0.3017 8360(58): 0.3020 \n",
      "EPOCH 58: train loss: 0.3020, test loss: 0.3139\n",
      "batch 1672(59): 0.3004 3344(59): 0.3011 5016(59): 0.3020 6688(59): 0.3022 8360(59): 0.3022 \n",
      "EPOCH 59: train loss: 0.3022, test loss: 0.3141\n",
      "batch 1672(60): 0.3006 3344(60): 0.3023 5016(60): 0.3027 6688(60): 0.3024 8360(60): 0.3022 \n",
      "EPOCH 60: train loss: 0.3022, test loss: 0.3141\n",
      "\n",
      "TRAIN:\n",
      "[[ 0.3514921   0.17811832]\n",
      " [ 0.14460019  0.32578939]]\n",
      "F1 = 0.6688\n",
      "\n",
      "TEST:\n",
      "[[ 0.3381052   0.20475194]\n",
      " [ 0.14847579  0.30866707]]\n",
      "F1 = 0.6361\n",
      "\n",
      "batch 1672(61): 0.2992 3344(61): 0.3006 5016(61): 0.3008 6688(61): 0.3013 8360(61): 0.3019 \n",
      "EPOCH 61: train loss: 0.3019, test loss: 0.3163\n",
      "batch 1672(62): 0.3034 3344(62): 0.3025 5016(62): 0.3022 6688(62): 0.3022 8360(62): 0.3020 \n",
      "EPOCH 62: train loss: 0.3019, test loss: 0.3155\n",
      "batch 1672(63): 0.3033 3344(63): 0.3033 5016(63): 0.3022 6688(63): 0.3023 8360(63): 0.3021 \n",
      "EPOCH 63: train loss: 0.3021, test loss: 0.3153\n",
      "batch 1672(64): 0.3019 3344(64): 0.3022 5016(64): 0.3023 6688(64): 0.3019 8360(64): 0.3020 \n",
      "EPOCH 64: train loss: 0.3020, test loss: 0.3138\n",
      "batch 1672(65): 0.3004 3344(65): 0.3021 5016(65): 0.3018 6688(65): 0.3021 8360(65): 0.3017 \n",
      "EPOCH 65: train loss: 0.3017, test loss: 0.3137\n",
      "batch 1672(66): 0.3020 3344(66): 0.3008 5016(66): 0.3014 6688(66): 0.3018 8360(66): 0.3014 \n",
      "EPOCH 66: train loss: 0.3014, test loss: 0.3142\n",
      "batch 1672(67): 0.3016 3344(67): 0.3021 5016(67): 0.3018 6688(67): 0.3017 8360(67): 0.3019 \n",
      "EPOCH 67: train loss: 0.3019, test loss: 0.3140\n",
      "batch 1672(68): 0.3016 3344(68): 0.3023 5016(68): 0.3011 6688(68): 0.3015 8360(68): 0.3016 \n",
      "EPOCH 68: train loss: 0.3017, test loss: 0.3143\n",
      "batch 1672(69): 0.3020 3344(69): 0.3018 5016(69): 0.3020 6688(69): 0.3015 8360(69): 0.3014 \n",
      "EPOCH 69: train loss: 0.3014, test loss: 0.3151\n",
      "batch 1672(70): 0.3005 3344(70): 0.3012 5016(70): 0.3020 6688(70): 0.3017 8360(70): 0.3016 \n",
      "EPOCH 70: train loss: 0.3016, test loss: 0.3137\n",
      "\n",
      "TRAIN:\n",
      "[[ 0.35051331  0.17909712]\n",
      " [ 0.14176841  0.32862117]]\n",
      "F1 = 0.6720\n",
      "\n",
      "TEST:\n",
      "[[ 0.33544531  0.20741184]\n",
      " [ 0.14465033  0.31249253]]\n",
      "F1 = 0.6397\n",
      "\n",
      "batch 1672(71): 0.3013 3344(71): 0.3017 5016(71): 0.3020 6688(71): 0.3021 8360(71): 0.3018 \n",
      "EPOCH 71: train loss: 0.3019, test loss: 0.3139\n",
      "batch 1672(72): 0.3013 3344(72): 0.3012 5016(72): 0.3016 6688(72): 0.3016 8360(72): 0.3017 \n",
      "EPOCH 72: train loss: 0.3016, test loss: 0.3149\n",
      "batch 1672(73): 0.2989 3344(73): 0.2991 5016(73): 0.3007 6688(73): 0.3010 8360(73): 0.3013 \n",
      "EPOCH 73: train loss: 0.3013, test loss: 0.3142\n",
      "batch 1672(74): 0.3011 3344(74): 0.3015 5016(74): 0.3019 6688(74): 0.3011 8360(74): 0.3013 \n",
      "EPOCH 74: train loss: 0.3013, test loss: 0.3142\n",
      "batch 1672(75): 0.3037 3344(75): 0.3027 5016(75): 0.3022 6688(75): 0.3021 8360(75): 0.3016 \n",
      "EPOCH 75: train loss: 0.3016, test loss: 0.3146\n",
      "batch 1672(76): 0.3013 3344(76): 0.3001 5016(76): 0.3008 6688(76): 0.3014 8360(76): 0.3015 \n",
      "EPOCH 76: train loss: 0.3015, test loss: 0.3158\n",
      "batch 1672(77): 0.2992 3344(77): 0.3012 5016(77): 0.3010 6688(77): 0.3008 8360(77): 0.3011 \n",
      "EPOCH 77: train loss: 0.3011, test loss: 0.3134\n",
      "batch 1672(78): 0.3011 3344(78): 0.3009 5016(78): 0.3013 6688(78): 0.3017 8360(78): 0.3013 \n",
      "EPOCH 78: train loss: 0.3013, test loss: 0.3129\n",
      "batch 1672(79): 0.3002 3344(79): 0.3011 5016(79): 0.3013 6688(79): 0.3014 8360(79): 0.3013 \n",
      "EPOCH 79: train loss: 0.3012, test loss: 0.3141\n",
      "batch 1672(80): 0.2998 3344(80): 0.3008 5016(80): 0.3007 6688(80): 0.3007 8360(80): 0.3009 \n",
      "EPOCH 80: train loss: 0.3009, test loss: 0.3144\n",
      "\n",
      "TRAIN:\n",
      "[[ 0.34531299  0.18429743]\n",
      " [ 0.13523812  0.33515145]]\n",
      "F1 = 0.6772\n",
      "\n",
      "TEST:\n",
      "[[ 0.32982666  0.21303048]\n",
      " [ 0.14001793  0.31712493]]\n",
      "F1 = 0.6424\n",
      "\n",
      "batch 1672(81): 0.2999 3344(81): 0.3003 5016(81): 0.3004 6688(81): 0.3009 8360(81): 0.3008 \n",
      "EPOCH 81: train loss: 0.3008, test loss: 0.3140\n",
      "batch 1672(82): 0.3029 3344(82): 0.3020 5016(82): 0.3018 6688(82): 0.3008 8360(82): 0.3009 \n",
      "EPOCH 82: train loss: 0.3008, test loss: 0.3132\n",
      "batch 1672(83): 0.3003 3344(83): 0.3005 5016(83): 0.3007 6688(83): 0.3009 8360(83): 0.3011 \n",
      "EPOCH 83: train loss: 0.3011, test loss: 0.3141\n",
      "batch 1672(84): 0.3003 3344(84): 0.3001 5016(84): 0.3002 6688(84): 0.3009 8360(84): 0.3008 \n",
      "EPOCH 84: train loss: 0.3008, test loss: 0.3143\n",
      "batch 1672(85): 0.3001 3344(85): 0.3003 5016(85): 0.3005 6688(85): 0.3006 8360(85): 0.3007 \n",
      "EPOCH 85: train loss: 0.3007, test loss: 0.3141\n",
      "batch 1672(86): 0.2983 3344(86): 0.3010 5016(86): 0.3005 6688(86): 0.3009 8360(86): 0.3007 \n",
      "EPOCH 86: train loss: 0.3007, test loss: 0.3139\n",
      "batch 1672(87): 0.2990 3344(87): 0.2991 5016(87): 0.3001 6688(87): 0.3003 8360(87): 0.3006 \n",
      "EPOCH 87: train loss: 0.3006, test loss: 0.3133\n",
      "batch 1672(88): 0.2996 3344(88): 0.3005 5016(88): 0.3008 6688(88): 0.3011 8360(88): 0.3008 \n",
      "EPOCH 88: train loss: 0.3008, test loss: 0.3134\n",
      "batch 1672(89): 0.3010 3344(89): 0.3006 5016(89): 0.3008 6688(89): 0.3008 8360(89): 0.3009 \n",
      "EPOCH 89: train loss: 0.3009, test loss: 0.3134\n",
      "batch 1672(90): 0.3015 3344(90): 0.3000 5016(90): 0.3002 6688(90): 0.3001 8360(90): 0.3005 \n",
      "EPOCH 90: train loss: 0.3005, test loss: 0.3146\n",
      "\n",
      "TRAIN:\n",
      "[[ 0.36151168  0.16809875]\n",
      " [ 0.15002466  0.32036492]]\n",
      "F1 = 0.6682\n",
      "\n",
      "TEST:\n",
      "[[ 0.3451584   0.19769874]\n",
      " [ 0.15502092  0.30212194]]\n",
      "F1 = 0.6314\n",
      "\n",
      "batch 1672(91): 0.3008 3344(91): 0.2993 5016(91): 0.3000 6688(91): 0.3003 8360(91): 0.3007 \n",
      "EPOCH 91: train loss: 0.3007, test loss: 0.3140\n",
      "batch 1672(92): 0.2991 3344(92): 0.3001 5016(92): 0.3004 6688(92): 0.3006 8360(92): 0.3007 \n",
      "EPOCH 92: train loss: 0.3007, test loss: 0.3136\n",
      "batch 1672(93): 0.3004 3344(93): 0.3004 5016(93): 0.3011 6688(93): 0.3008 8360(93): 0.3010 \n",
      "EPOCH 93: train loss: 0.3009, test loss: 0.3137\n",
      "batch 1672(94): 0.3002 3344(94): 0.3011 5016(94): 0.3010 6688(94): 0.3012 8360(94): 0.3009 \n",
      "EPOCH 94: train loss: 0.3009, test loss: 0.3133\n",
      "batch 1672(95): 0.3000 3344(95): 0.3001 5016(95): 0.3004 6688(95): 0.3009 8360(95): 0.3005 \n",
      "EPOCH 95: train loss: 0.3005, test loss: 0.3141\n",
      "batch 1672(96): 0.3005 3344(96): 0.2997 5016(96): 0.3000 6688(96): 0.3003 8360(96): 0.3004 \n",
      "EPOCH 96: train loss: 0.3004, test loss: 0.3149\n",
      "batch 1672(97): 0.3000 3344(97): 0.2997 5016(97): 0.3002 6688(97): 0.3004 8360(97): 0.3006 \n",
      "EPOCH 97: train loss: 0.3006, test loss: 0.3142\n",
      "batch 1672(98): 0.2998 3344(98): 0.2994 5016(98): 0.2998 6688(98): 0.3005 8360(98): 0.3004 \n",
      "EPOCH 98: train loss: 0.3005, test loss: 0.3148\n",
      "batch 1672(99): 0.3016 3344(99): 0.3005 5016(99): 0.2999 6688(99): 0.3002 8360(99): 0.3004 \n",
      "EPOCH 99: train loss: 0.3004, test loss: 0.3156\n",
      "batch 1672(100): 0.2999 3344(100): 0.2998 5016(100): 0.2997 6688(100): 0.2998 8360(100): 0.3000 \n",
      "EPOCH 100: train loss: 0.3000, test loss: 0.3133\n",
      "\n",
      "TRAIN:\n",
      "[[ 0.35942707  0.17018336]\n",
      " [ 0.14669227  0.32369731]]\n",
      "F1 = 0.6714\n",
      "\n",
      "TEST:\n",
      "[[ 0.34462044  0.1982367 ]\n",
      " [ 0.15167364  0.30546922]]\n",
      "F1 = 0.6358\n",
      "\n",
      "batch 1672(101): 0.2994 3344(101): 0.3003 5016(101): 0.2999 6688(101): 0.2999 8360(101): 0.3001 \n",
      "EPOCH 101: train loss: 0.3001, test loss: 0.3134\n",
      "batch 1672(102): 0.2986 3344(102): 0.2993 5016(102): 0.3000 6688(102): 0.2999 8360(102): 0.3002 \n",
      "EPOCH 102: train loss: 0.3002, test loss: 0.3142\n",
      "batch 1672(103): 0.2981 3344(103): 0.2994 5016(103): 0.3002 6688(103): 0.3004 8360(103): 0.3001 \n",
      "EPOCH 103: train loss: 0.3001, test loss: 0.3143\n",
      "batch 1672(104): 0.3004 3344(104): 0.3000 5016(104): 0.2997 6688(104): 0.3001 8360(104): 0.3001 \n",
      "EPOCH 104: train loss: 0.3001, test loss: 0.3143\n",
      "batch 1672(105): 0.2994 3344(105): 0.3000 5016(105): 0.2995 6688(105): 0.2997 8360(105): 0.3005 \n",
      "EPOCH 105: train loss: 0.3005, test loss: 0.3123\n",
      "batch 1672(106): 0.3001 3344(106): 0.3010 5016(106): 0.3009 6688(106): 0.3003 8360(106): 0.3002 \n",
      "EPOCH 106: train loss: 0.3002, test loss: 0.3135\n",
      "batch 1672(107): 0.3010 3344(107): 0.3011 5016(107): 0.3008 6688(107): 0.3006 8360(107): 0.3002 \n",
      "EPOCH 107: train loss: 0.3002, test loss: 0.3136\n",
      "batch 1672(108): 0.2994 3344(108): 0.2994 5016(108): 0.3003 6688(108): 0.2998 8360(108): 0.3000 \n",
      "EPOCH 108: train loss: 0.3000, test loss: 0.3138\n",
      "batch 1672(109): 0.2986 3344(109): 0.3006 5016(109): 0.3000 6688(109): 0.2997 8360(109): 0.3000 \n",
      "EPOCH 109: train loss: 0.3000, test loss: 0.3133\n",
      "batch 1672(110): 0.2990 3344(110): 0.3000 5016(110): 0.3000 6688(110): 0.2993 8360(110): 0.2996 \n",
      "EPOCH 110: train loss: 0.2996, test loss: 0.3145\n",
      "\n",
      "TRAIN:\n",
      "[[ 0.34799534  0.18161509]\n",
      " [ 0.13589564  0.33449394]]\n",
      "F1 = 0.6781\n",
      "\n",
      "TEST:\n",
      "[[ 0.33093246  0.21192469]\n",
      " [ 0.14136282  0.31578004]]\n",
      "F1 = 0.6413\n",
      "\n",
      "batch 1672(111): 0.3011 3344(111): 0.2999 5016(111): 0.2999 6688(111): 0.3000 8360(111): 0.3000 \n",
      "EPOCH 111: train loss: 0.3000, test loss: 0.3144\n",
      "batch 1672(112): 0.3005 3344(112): 0.3019 5016(112): 0.3017 6688(112): 0.3009 8360(112): 0.3004 \n",
      "EPOCH 112: train loss: 0.3004, test loss: 0.3157\n",
      "batch 1672(113): 0.3008 3344(113): 0.3005 5016(113): 0.3007 6688(113): 0.3001 8360(113): 0.3001 \n",
      "EPOCH 113: train loss: 0.3000, test loss: 0.3134\n",
      "batch 1672(114): 0.2994 3344(114): 0.2989 5016(114): 0.2992 6688(114): 0.2998 8360(114): 0.2999 \n",
      "EPOCH 114: train loss: 0.2999, test loss: 0.3149\n",
      "batch 1672(115): 0.2979 3344(115): 0.2990 5016(115): 0.2993 6688(115): 0.2993 8360(115): 0.2995 \n",
      "EPOCH 115: train loss: 0.2995, test loss: 0.3139\n",
      "batch 1672(116): 0.2975 3344(116): 0.2983 5016(116): 0.2990 6688(116): 0.2998 8360(116): 0.3000 \n",
      "EPOCH 116: train loss: 0.3000, test loss: 0.3129\n",
      "batch 1672(117): 0.2986 3344(117): 0.2988 5016(117): 0.2989 6688(117): 0.2998 8360(117): 0.2999 \n",
      "EPOCH 117: train loss: 0.2999, test loss: 0.3144\n",
      "batch 1672(118): 0.2999 3344(118): 0.3004 5016(118): 0.2997 6688(118): 0.2997 8360(118): 0.2997 \n",
      "EPOCH 118: train loss: 0.2997, test loss: 0.3130\n",
      "batch 1672(119): 0.3011 3344(119): 0.3007 5016(119): 0.3000 6688(119): 0.2996 8360(119): 0.2995 \n",
      "EPOCH 119: train loss: 0.2995, test loss: 0.3134\n",
      "batch 1672(120): 0.2997 3344(120): 0.3000 5016(120): 0.2998 6688(120): 0.2998 8360(120): 0.2999 \n",
      "EPOCH 120: train loss: 0.2999, test loss: 0.3128\n",
      "\n",
      "TRAIN:\n",
      "[[ 0.36530731  0.16430311]\n",
      " [ 0.15153394  0.31885563]]\n",
      "F1 = 0.6688\n",
      "\n",
      "TEST:\n",
      "[[ 0.34967125  0.19318589]\n",
      " [ 0.15621638  0.30092648]]\n",
      "F1 = 0.6327\n",
      "\n",
      "batch 1672(121): 0.2978 3344(121): 0.2992 5016(121): 0.2999 6688(121): 0.2998 8360(121): 0.2997 \n",
      "EPOCH 121: train loss: 0.2997, test loss: 0.3145\n",
      "batch 1672(122): 0.2997 3344(122): 0.2994 5016(122): 0.2995 6688(122): 0.2995 8360(122): 0.2993 \n",
      "EPOCH 122: train loss: 0.2993, test loss: 0.3137\n",
      "batch 1672(123): 0.2980 3344(123): 0.2992 5016(123): 0.2993 6688(123): 0.2995 8360(123): 0.2994 \n",
      "EPOCH 123: train loss: 0.2994, test loss: 0.3147\n",
      "batch 1672(124): 0.2986 3344(124): 0.2991 5016(124): 0.2997 6688(124): 0.2999 8360(124): 0.2999 \n",
      "EPOCH 124: train loss: 0.2999, test loss: 0.3138\n",
      "batch "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-34459ad9d8a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mibatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}({}): {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mibatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/envs/ix/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iepoch0 = iepoch\n",
    "while iepoch != iepoch0 + NEPOCHS:\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_samples = 0\n",
    "    print(\"batch\", end=\" \")\n",
    "    for ibatch, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        z = model(Variable(batch[:, 2:-1]))\n",
    "        y = Variable(batch[:, -1]).resize(batch.size(0), 1)\n",
    "        weight = (y * class_weight[1] + (1 - y) * class_weight[0])/(class_weight[0] + class_weight[1])\n",
    "        loss = F.binary_cross_entropy(z, y, weight)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0] * batch.size(0)\n",
    "        train_samples += batch.size(0)\n",
    "        optimizer.step()\n",
    "        if (ibatch + 1) % (len(trainset) // BATCH_SIZE // 5) == 0:\n",
    "            print(\"{}({}): {:.4f}\".format(ibatch + 1, iepoch + 1, train_loss / train_samples), end=\" \")\n",
    "    print()\n",
    "    train_loss /= train_samples\n",
    "        \n",
    "    model.eval()\n",
    "    z = model(Variable(testset[:, 2:-1]))\n",
    "    y = Variable(testset[:, -1]).resize(len(testset), 1)\n",
    "    weight = (y * class_weight[1] + (1 - y) * class_weight[0])/(class_weight[0] + class_weight[1])\n",
    "    loss = F.binary_cross_entropy(z, y, weight)\n",
    "    print(\"EPOCH {}: train loss: {:.4f}, test loss: {:.4f}\".format(iepoch + 1, train_loss, loss.data[0]))  \n",
    "    iepoch += 1\n",
    "    \n",
    "    if iepoch % 10 == 0:\n",
    "        train_true, train_pred = truepred(model, trainset)\n",
    "        test_true, test_pred = truepred(model, testset)\n",
    "\n",
    "        print(\"\\nTRAIN:\\n{}\\nF1 = {:.4f}\\n\\nTEST:\\n{}\\nF1 = {:.4f}\\n\"\n",
    "              .format(confusion_matrix(train_true, train_pred)/len(trainset),\n",
    "                      f1_score(train_true, train_pred),\n",
    "                       confusion_matrix(test_true, test_pred)/len(testset), \n",
    "                      f1_score(test_true, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(MODELS, \"changepoints.model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
